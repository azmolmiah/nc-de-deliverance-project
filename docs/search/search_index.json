{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Deliverance Data Engineering Project","text":"<p>Welcome to the Deliverance Data Engineering Project documentation! This project showcases a scalable data pipeline using AWS services for data extraction, transformation, and loading. Utilizing AWS Lambda, Terraform, and GitHub Actions, we ensure efficient and automated data processing. Explore this documentation to learn about our architecture, implementation, and setup.</p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>The primary objective of this project is to showcase skills and knowledge in Python, SQL, database modeling, AWS, operational practices, and Agile methodologies. The project involves the following key components:</p> <ol> <li>Data Ingestion: A Python application running on AWS Lambda that continually ingests data from the <code>totesys</code> database and stores it in an S3 \"ingestion\" bucket.</li> <li>Data Processing: Another Python application running on AWS Lambda that remodels the ingested data into a predefined schema suitable for a data warehouse and stores the processed data in Parquet format in an S3 \"processed\" bucket.</li> <li>Data Loading: A Python application running on AWS Lambda that loads the processed data into a prepared data warehouse hosted on AWS at defined intervals.</li> <li>Monitoring and Alerting: Comprehensive logging, monitoring, and alerting mechanisms using AWS CloudWatch to track the progress, detect failures, and trigger email notifications.</li> <li>Data Visualization: A QuickSight dashboard that displays useful data from the data warehouse.</li> <li>Infrastructure as Code: Automated deployment of the entire infrastructure using Terraform and a CI/CD pipeline with GitHub Actions. Dev, test and prod environments.</li> </ol>"},{"location":"#repository-structure","title":"Repository Structure","text":"<p>The repository is organized as follows:</p> <pre><code>.\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 conventions/\n\u2502   \u251c\u2500\u2500 ci-cd.md\n\u2502   \u251c\u2500\u2500 code-review.md\n\u2502   \u251c\u2500\u2500 docs-and-comments.md\n\u2502   \u251c\u2500\u2500 pull-request.md\n\u2502   \u251c\u2500\u2500 terraform.md\n\u2502   \u2514\u2500\u2500 testing.md\n\u251c\u2500\u2500 db/\n\u2502   \u251c\u2500\u2500 connection.py\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 run_schema.py\n\u2502   \u251c\u2500\u2500 run_seed.py\n\u2502   \u251c\u2500\u2500 schema.sql\n\u2502   \u2514\u2500\u2500 seed.py\n\u251c\u2500\u2500 dev-db-terraform/\n\u2502   \u251c\u2500\u2500 dev_db.tf\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 python/\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 tests/\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 deploy.sh\n\u251c\u2500\u2500 specifications/\n\u2502   \u251c\u2500\u2500 Deliverance_ETL_architecture_diagram.png\n\u2502   \u251c\u2500\u2500 Deliverance_ETL_architecture_diagram.svg\n\u2502   \u251c\u2500\u2500 S3_Data_Storage_Specification.md\n\u2502   \u251c\u2500\u2500 ingestion_lambda_spec.md\n\u2502   \u251c\u2500\u2500 project_plan.md\n\u2502   \u251c\u2500\u2500 specifiction.md\n\u2502   \u2514\u2500\u2500 processing_lambda_spec.md\n\u2514\u2500\u2500 terraform/\n    \u251c\u2500\u2500 data.tf\n    \u251c\u2500\u2500 dev.tfvars\n    \u251c\u2500\u2500 eventbridge.tf\n    \u251c\u2500\u2500 iam.tf\n    \u251c\u2500\u2500 lambda.tf\n    \u251c\u2500\u2500 main.tf\n    \u251c\u2500\u2500 prod.tfvars\n    \u251c\u2500\u2500 s3.tf\n    \u251c\u2500\u2500 test.tfvars\n    \u2514\u2500\u2500 variables.tf\n</code></pre> <ul> <li><code>terraform/</code>: Contains Terraform configuration files for provisioning the AWS infrastructure.</li> <li><code>python/</code>: Contains the source code for the Python Lambda functions responsible for data ingestion, processing, and loading. Includes unit tests.</li> <li><code>.github/workflows/</code>: Contains GitHub Actions workflows for continuous integration and deployment.</li> <li><code>README.md</code>: This file, providing an overview of the project and instructions for setup and deployment.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with the project, follow these steps:</p> <ol> <li>Clone the repository: <code>git clone https://github.com/your-username/totesys-data-engineering.git</code></li> <li>Install the required dependencies (e.g., Terraform, AWS CLI, Python, etc.).</li> <li>Configure your AWS credentials and set up the necessary IAM roles and policies.</li> <li>Customize the Terraform configuration files in the <code>terraform/</code> directory to match your AWS account and desired settings.</li> <li>Deploy the infrastructure using Terraform: <code>terraform init</code> and <code>terraform apply</code>.</li> <li>Set up the CI/CD pipeline by configuring the GitHub Actions workflows in the <code>.github/workflows/</code> directory.</li> <li>Commit and push your changes to the repository to trigger the CI/CD pipeline and deploy the Lambda functions.</li> <li>Monitor the pipeline execution and check the CloudWatch logs for any issues or failures.</li> <li>Once the deployment is successful, you can trigger the data ingestion process and observe the data flow through the pipeline.</li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions to this project are welcome. If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Northcoders for providing the project specification and guidance.</li> <li>AWS Documentation for the comprehensive documentation on AWS services.</li> <li>Terraform Documentation for the Terraform documentation and examples.</li> </ul>"},{"location":"ingestion/lambda_utils/","title":"get_timestamp","text":"<pre><code>Return timestamp showing most recent entry from the given table\nthat has been processed by the ingestion lambda.\n\nArgs:\n    table_name (str): table name to get timestamp for\n    client (boto3 SSM Client)\n\nRaises:\n    KeyError: table_name does not exist\n    ConnectionError : connection issue to parameter store\n\nReturns:\n    timestamp (datetime timestamp) : stored timestamp of most recent\n    ingested data for given table\n</code></pre>"},{"location":"ingestion/lambda_utils/#write_timestamp","title":"write_timestamp","text":"<pre><code>Writes timestamp to parameter store for given table\n\nArgs:\n    timestamp (timestamp) : timestamp of latest extracted data\n    table_name (str) : table name to store timestamp for\n    client (boto3 SSM Client) : client passed in to avoid recreating for each invocation\n\nRaises:\n    ConnectionError : connection issue to parameter store\n\nReturns:\n    None\n</code></pre>"},{"location":"ingestion/lambda_utils/#collect_table_data","title":"collect_table_data","text":"<pre><code>Returns all data from a table newer than most recent timestamp\n\nArgs:\n    table_name (string)\n    timestamp (timestamp)\n    db_conn (pg8000 database connection)\n\nRaises:\n    KeyError: table_name does not exist\n    ConnectionError : connection issue to parameter store\n\n\nReturns:\n    table_data (list) : list of dictionaries\n        all data in table, one dictionary\n        per row keys will be column headings\n</code></pre>"},{"location":"ingestion/lambda_utils/#find_latest_timestamp","title":"find_latest_timestamp","text":"<pre><code>Iterates over data from one database table and returns the most recent timestamp\n\nArgs:\n    table_data (list) : list of dictionaries representing rows of the table\n    columns (list[str], optional keyword) : columns to search for timestamps.\n        Defauts to [\"last_updated\"]\n\nRaises:\n    KeyError: columns do not exist\n\nReturns:\n    most_recent_timestamp (timestamp) : from list returns most recent\n        timestamp from created_at/updated_at values\n</code></pre>"},{"location":"ingestion/lambda_utils/#write_table_data_to_s3","title":"write_table_data_to_s3","text":"<pre><code>Write file to S3 bucket as Json lines format\n\nArgs:\n    table_name (string)\n    table_data (list) : list of dictionaries all data in table,\n        one dictionary per row keys will be column headings\n    s3_client (boto3 s3 client)\n\n\nRaises:\n    FileExistsError: S3 object already exists with the same name\n    ConnectionError : connection issue to S3 bucket\n\nReturns:\n    key (str): The S3 object key the data is written to\n</code></pre>"},{"location":"ingestion/lambda_utils/#get_seq_id","title":"get_seq_id","text":"<pre><code>From parameter store retrieves table_name : sequential_id key value pair\n\nArgs:\n    table_name (string)\n\n\nRaises:\n    KeyError: table_name does not exist\n    ConnectionError : connection issue to parameter store\n\nReturns:\n    sequential_id(int)\n</code></pre>"},{"location":"ingestion/lambda_utils/#write_seq_id","title":"write_seq_id","text":"<pre><code>To parameter store write table_name : sequential_id key value pair\n-- checks sequential_id is one greater than previous sequential_id\n\nArgs:\n    table_name (string)\n    sequential_id(int)\n\n\nRaises:\n    KeyError: table_name does not exist\n    ConnectionError : connection issue to parameter store\n\nReturns:\n    None\n</code></pre>"},{"location":"loading/lambda_utils/","title":"retrieve_processed_data","text":"<pre><code>Retrieves parquet data from s3 bucket, given a key\n\nArgs:\n    bucket_name (string)\n    object_key (string)\n    s3_client (boto3 s3 client)\n\nRaises:\n    KeyError: object does not exist\n    ConnectionError : connection issue to parameter store\n\n\nReturns:\n    table_data (pandas dataframe)\n</code></pre>"},{"location":"loading/lambda_utils/#write_table_data_to_warehouse","title":"write_table_data_to_warehouse","text":"<pre><code>Write pandas dataframe to database\n\nArgs:\n    data_frame (pd.DataFrame)\n    table_name (string)\n    db (pg8000 Connection)\n\nReturns:\n    response: database response\n</code></pre>"},{"location":"loading/lambda_utils/#create_dim_date","title":"create_dim_date","text":"<pre><code>Creates a table of dates in the given range\nwith columns for:\n    - year\n    - month\n    - month name\n    - day of month\n    - day of year\n    - day of week\n    - quarter\n\n\nArgs:\n    start_date (datetime)\n    end_date (datetime)\n\nReturns:\n    dates (pandas dataframe)\n</code></pre>"},{"location":"loading/lambda_utils/#get_timestamp","title":"get_timestamp","text":"<pre><code>Return timestamp showing most recent entry from the given table\nthat has been processed by the ingestion lambda.\n\nArgs:\n    table_name (str): table name to get timestamp for\n    client (boto3 SSM Client)\n\nRaises:\n    KeyError: table_name does not exist\n    ConnectionError : connection issue to parameter store\n\nReturns:\n    timestamp (datetime timestamp) : stored timestamp of most recent\n    ingested data for given table\n</code></pre>"},{"location":"loading/lambda_utils/#write_timestamp","title":"write_timestamp","text":"<pre><code>Writes timestamp to parameter store for given table\n\nArgs:\n    timestamp (timestamp) : timestamp of latest extracted data\n    table_name (str) : table name to store timestamp for\n    client (boto3 SSM Client) : client passed in to avoid recreating for each invocation\n\nRaises:\n    ConnectionError : connection issue to parameter store\n\nReturns:\n    None\n</code></pre>"},{"location":"processing/lambda_utils/","title":"retrieve_data","text":"<pre><code>Load data from an s3 object (json lines) into a pandas dataframe\n\nArgs:\n    bucket_name (str): bucket where the object is stored\n    object_key (str): key of data object\n    client (boto3 s3 Client)\n\nRaises:\n    KeyError: table_name does not exist\n    ConnectionError : connection issue to parameter store\n\nReturns:\n    data: pandas dataframe\n</code></pre>"},{"location":"processing/lambda_utils/#transform_sales_order","title":"transform_sales_order","text":"<pre><code>Transform loaded sales order data into star schema\n    - splits out time and date data into separate columns\n    - renames staff_id to sales_staff_id\n    - removes unwanted columns\n\nArgs:\n    sales_order_df (pandas dataframe): original data\n\nReturns:\n    data (pandas dataframe): transformed data\n</code></pre>"},{"location":"processing/lambda_utils/#transform_purchase_order","title":"transform_purchase_order","text":"<pre><code>Transform loaded purchase order data into star schema\n    - splits out time and date data into separate columns\n    - removes unwanted columns\n\nArgs:\n    purchase_order_df (pandas dataframe): original data\n\nReturns:\n    data (pandas dataframe): transformed data\n</code></pre>"},{"location":"processing/lambda_utils/#transform_payment","title":"transform_payment","text":"<pre><code>Transform loaded payment data into star schema\n    - splits out time and date data into separate columns\n    - renames columns to suit star schema\n\nArgs:\n    payment_df (pandas dataframe): original data\n\nReturns:\n    data (pandas dataframe): transformed data\n</code></pre>"},{"location":"processing/lambda_utils/#transform_staff","title":"transform_staff","text":"<pre><code>Transform loaded staff order data into star schema\n    - adds a location by reference to department table\n\nArgs:\n    staff_df (pandas dataframe): original data\n    department_df (pandas dataframe): department data for location\n\nReturns:\n    data (pandas dataframe): transformed data\n</code></pre>"},{"location":"processing/lambda_utils/#transform_location","title":"transform_location","text":"<pre><code>Transform loaded address order data into star schema\n    - renames address_id to location_id\n\nArgs:\n    address_df (pandas dataframe): original data\n\nReturns:\n    data (pandas dataframe): transformed data\n</code></pre>"},{"location":"processing/lambda_utils/#transform_payment_type","title":"transform_payment_type","text":"<pre><code>Transform loaded payment type data into star schema\n\nArgs:\n    payment_type_df (pandas dataframe)\n\nReturns:\n    data (pandas dataframe): transformed data\n</code></pre>"},{"location":"processing/lambda_utils/#transform_transaction","title":"transform_transaction","text":"<pre><code>Transform loaded transaction data into star schema\n\nArgs:\n    transaction_df (pandas dataframe): original write_data_to_s3\n\nReturns:\n    data (pandas dataframe): transformed data\n</code></pre>"},{"location":"processing/lambda_utils/#transform_currency","title":"transform_currency","text":"<pre><code>Transform loaded currency data into star schema\n    - adds currency names from currency codes\n    - remove unwanted last_updated column\n\nArgs:\n    currency_df (pandas dataframe): original data\n\nReturns:\n    data (pandas dataframe): transformed data\n</code></pre>"},{"location":"processing/lambda_utils/#transform_design","title":"transform_design","text":"<pre><code>Transform loaded design data into star schema\n    - remove unwanted columns\n\nArgs:\n    design_df (pandas dataframe): original data\n\nReturns:\n    data (pandas dataframe): transformed data\n</code></pre>"},{"location":"processing/lambda_utils/#transform_counterparty","title":"transform_counterparty","text":"<pre><code>Transform loaded counterparty data into star schema\n    - lookup address in address data and add to table\n    - renames columns to suit star schema\n    - removes unwanted columns\n\nArgs:\n    counterparty_df (pandas dataframe): original data\n    address_df (pandas dataframe): address data for reference\n\nReturns:\n    data (pandas dataframe): transformed data\n</code></pre>"},{"location":"processing/lambda_utils/#write_data_to_s3","title":"write_data_to_s3","text":"<pre><code>Write dataframe to S3 bucket in Parquet format\n\nArgs:\n    df (pd.DataFrame): DataFrame to write\n    table_name (string)\n    bucket_name (string)\n    packet_id (string)\n    s3_client (boto3 s3 client)\n\nRaises:\n    FileExistsError: S3 object already exists with the same name\n    ConnectionError : connection issue to S3 bucket\n\nReturns:\n    key: The S3 object key the data is written to\n</code></pre>"}]}